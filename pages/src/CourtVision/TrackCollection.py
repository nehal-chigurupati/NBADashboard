# -*- coding: utf-8 -*-
"""TrackingCollection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17GbnI6kclfn6TrND9oK329Za-uAmB99J
"""

import os
import random
import numpy as np
import cv2
from ultralytics import YOLO
from pages.src.CourtVision.tracker import Tracker
from sklearn.cluster import KMeans
import json
import streamlit as st
from stqdm import stqdm

def dump_dict_to_json(my_dict, file_path):
    with open(file_path, 'w') as json_file:
        json.dump(my_dict, json_file)

def visual_tracking(video_path, video_out_path):
  cap = cv2.VideoCapture(video_path)
  ret, frame = cap.read()
  cap_out = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*'MP4V'), cap.get(cv2.CAP_PROP_FPS), (frame.shape[1], frame.shape[0]))

  model = YOLO("yolov8n.pt")
  tracker = Tracker()

  #10 players + 3 refs
  colors = [(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for j in range(13)]

  frame_num = 0
  while ret:
      frame_num += 1
      results = model(frame)

      for person in results:
          detections = []
          for r in person.boxes.data.tolist():
              x1, y1, x2, y2, confidence, class_type = r
              x1 = int(x1)
              x2 = int(x2)
              y1 = int(y1)
              y2 = int(y2)
              class_type = int(class_type)
              detections.append([x1, y1, x2, y2, confidence])

          tracker.update(frame, detections)

          for track in tracker.tracks:

              bbox = track.bbox
              track_id = track.track_id
              x1, y1, x2, y2 = bbox

              cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (colors[track_id % len(colors)]))
              cv2.putText(frame, str(track_id),(int(x1), int(y1)),0, 5e-3 * 200, (colors[track_id % len(colors)]),2)


      cv2.imshow('frame', frame)
      cv2.waitKey(25)
      cap_out.write(frame)
      ret, frame = cap.read()

  cap.release()
  cap_out.release()
  cv2.destroyAllWindows()

def track_with_color_labels(video_path, video_out_path, kmeans_model, cluster_one_label, cluster_two_label):
    cap = cv2.VideoCapture(video_path)
    ret, frame = cap.read()
    cap_out = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*'mp4v'), cap.get(cv2.CAP_PROP_FPS), (frame.shape[1], frame.shape[0]))

    model = YOLO("yolov8n.pt")
    tracker = Tracker()

    #10 players + 3 refs
    colors = [(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for j in range(13)]

    frame_num = 0
    while ret:
        frame_num += 1
        results = model(frame)

        for person in results:
            detections = []
            for r in person.boxes.data.tolist():
                x1, y1, x2, y2, confidence, class_type = r
                x1 = int(x1)
                x2 = int(x2)
                y1 = int(y1)
                y2 = int(y2)
                class_type = int(class_type)
                detections.append([x1, y1, x2, y2, confidence])

            tracker.update(frame, detections)

            for track in tracker.tracks:

                bbox = track.bbox
                track_id = track.track_id
                x1, y1, x2, y2 = bbox

                #Extract jersey color
                cent_x = int((x1 + x2) / 2)
                cent_y = int((y1+y2) / 2)
                pixel_color = frame[cent_y, cent_x]

                R = int(pixel_color[2])
                G = int(pixel_color[1])
                B = int(pixel_color[0])
                cluster = kmeans_model.predict(np.array([R, G, B]).reshape(1, -1))

                if cluster == 0:
                    team = cluster_one_label
                elif cluster == 1:
                    team = cluster_two_label
                else:
                    team = None
                    

                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (colors[track_id % len(colors)]))
                #cv2.putText(frame, str(track_id),(int(x1), int(y1)),0, 5e-3 * 200, (colors[track_id % len(colors)]),2)
                cv2.putText(frame, str(team),(int(x1), int(y1)),0, 5e-3 * 200, (colors[track_id % len(colors)]),2)


        cv2.imshow('frame', frame)
        cv2.waitKey(25)
        cap_out.write(frame)
        ret, frame = cap.read()

    cap.release()
    cap_out.release()
    cv2.destroyAllWindows()

def generate_tracking_data(file, with_video=False, video_out_path=""):
    cap = cv2.VideoCapture(file)
    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    ret, frame = cap.read()

    if with_video:
        cap_out = cv2.VideoWriter(video_out_path, cv2.VideoWriter_fourcc(*'H264'), cap.get(cv2.CAP_PROP_FPS), (frame.shape[1], frame.shape[0]))
    
    player_ids = {}
    player_ids["HOME"] = [None] * 5
    player_ids["AWAY"] = [None] * 5

    model = YOLO("yolov8n.pt")
    tracker = Tracker()

    colors = [(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for j in range(13)]

    json_out = {}
    frame_num = 0
    for i in stqdm(range(num_frames)):
        frame_num += 1
        results = model(frame)

        cv2.imwrite('frame_img.jpg', frame)

        prediction_text = []

        for person in results:
            detections = []
            for r in person.boxes.data.tolist():
                x1, y1, x2, y2, confidence, class_type = r
                x1 = int(x1)
                x2 = int(x2)
                y1 = int(y1)
                y2 = int(y2)

                class_type = int(class_type)
                detections.append([x1, y1, x2, y2, confidence])

            tracker.update(frame, detections)

            for track in tracker.tracks:

                bbox = track.bbox
                track_id = track.track_id
                x1, y1, x2, y2 = bbox

                #Extract jersey color
                cent_x = int((x1 + x2) / 2)
                cent_y = int((y1+y2) / 2)
                pixel_color = frame[cent_y, cent_x]

                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (colors[track_id % len(colors)]))
                cv2.putText(frame, str(track_id),(int(x1), int(y1)),0, 5e-3 * 200, (colors[track_id % len(colors)]),2)

                if track_id not in json_out.keys():
                    json_out[track_id] = {
                        "FRAME_NUM": [],
                        "X1": [],
                        "Y1": [],
                        "X2": [],
                        "Y2": [],
                        "REC_NUM": [],
                        "R": [],
                        "G": [],
                        "B": []
                    }

                json_out[track_id]["FRAME_NUM"].append(frame_num)
                json_out[track_id]["X1"].append(int(x1))
                json_out[track_id]["Y1"].append(int(y1))
                json_out[track_id]["X2"].append(int(x2))
                json_out[track_id]["Y2"].append(int(y2))
                json_out[track_id]["R"].append(int(pixel_color[2]))
                json_out[track_id]["G"].append(int(pixel_color[1]))
                json_out[track_id]["B"].append(int(pixel_color[0]))

        #cv2.imshow('frame', frame)
        #cv2.waitKey(5)
                
        if with_video:
            cap_out.write(frame)
        
        ret, frame = cap.read()

    cap.release()
    cv2.destroyAllWindows()

    json_out["META"] = {"NUM_FRAMES_IN_VIDEO": num_frames}

    return json_out
